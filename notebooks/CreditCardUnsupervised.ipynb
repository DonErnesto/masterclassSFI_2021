{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DonErnesto/masterclassSFI_2021/blob/main/notebooks/CreditCardUnsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction (together, 5 minutes)\n",
    "\n",
    "In this \"notebook\", we can run Python commands, make plots, and make notes. \n",
    "\n",
    "The purpose of this notebook is to guide you through some approaches to outlier detection using Python, and give you an impression of what the various algorithms do. \n",
    "\n",
    "Note that there are two types of cells in this notebook: Markdown cells (that contain text, like this one), and Code cells (that execute some code, like the next cell). \n",
    "\n",
    "By clicking the \"Play-button\" on a cell, we execute that code cell. Lines within code cells that start with a \"#\" are Python comments, and not executed. \n",
    "\n",
    "Your input is required whenever there is a Question (in that case: write in the Markdown cell) or whenever you find some 'xxxxx' in the code cell (in this case, some code needs to be fixed or completed).\n",
    "\n",
    "We start by importing our outlier data, by executing the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data import from Github\n",
    "!curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/X_unsupervised.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the \"pandas\" package for data handling and manipulation, and later \"scikit-learn\" (imported with \"sklearn\") for various outlier detection algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package import: pandas for data handling and manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# A small hack: \"monkey-patching\" the DataFrame class to add column-wise normalization as a method\n",
    "def normalize_columns(self,):\n",
    "    return (self - self.mean()) / self.std()\n",
    "\n",
    "pd.DataFrame.normalize_columns = normalize_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the data in a so-called DataFrame (a pandas object), and inspect it by plotting the N-top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X_unsupervised.csv.zip')\n",
    "# .head() returns a DataFrame, that consists of the first N (default: N=5) rows \n",
    "# of the DataFrame it is applied on\n",
    "X.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data describes credit card transactions, one transaction per row. \n",
    "\n",
    "As you may notice, all features are numeric. All Vx features were generated by compressing the original data using a mathematical operation called PCA. In reality, we always have to convert our data to a purely numerical form (however, we generally want to avoid losing touch of the meaning of the attributes, for instance reasons of explainability).\n",
    "\n",
    "In this case, it is advantageous because little pre-processing or interpretation is needed, and we can feed the data directly into any algorithm, which will save us time. \n",
    "\n",
    "Let us first determine the dimensions of the DataFrame (note that the first dimension goes along the rows, the second along columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any realistic situation, we would not have access to labels (otherwise, we would be using a supervised approach) and typically know nothing about the fraction of positives. We will already give one fact away: the fraction of positive labels is about 0.3%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let's demonstrate some dataframe operations, with a smaller demonstration dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some useful pandas DataFrame methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate the hints with a smaller dataframe (the first 5 rows):\n",
    "small_df = X.head(5).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.drop()** \n",
    "\n",
    "The .drop(columns=[...]) method can be applied on a DataFrame to drop one or more rows or columns, and returns itself (i.e.: a DataFrame). \n",
    "\n",
    "Example usage to delete (\"drop\") one or more columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_df.drop(columns=['V1', 'V5']) # This drops the V1 and V5 column s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.abs()** \n",
    "\n",
    "The .abs() method can be applied on a DataFrame (or Series) to convert absolute numerical values, and returns itself (i.e., a DataFrame). \n",
    "\n",
    "Example usage for .abs():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.max(axis=1), .sum(axis=1), .mean(axis=1)**\n",
    "\n",
    "These methods can be applied on a DataFrame to do row-wise operations. They all returna a Series (with as many rows as the DataFrame it was applied on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.max(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**.normalize_columns()**\n",
    "\n",
    "We added this method to our DataFrame in the beginning. It performs column-wise normalization (i.e.: after this operation, the column-wise mean is zero, and the column-wise variance is one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "small_df.normalize_columns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other useful operations: selecting single and multiple columns\n",
    "\n",
    "Generally, this returns a DataFrame when selecting multiple columns, and a Series when selecting a single columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Selecting a single column by its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single column:\n",
    "small_df['Amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Selecting multiple columns with their numerical index using .iloc: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 5 columns:\n",
    "small_df.iloc[:, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns execpt the last one:\n",
    "small_df.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many pandas DataFrame methods return a DataFrame, on which we can apply another function. \n",
    "Applying a method on the result of another method is called \"chaining\". We can for instance first drop a column, then use normalize_columns() (our home-made addition) to normalize the columns), then .abs() to convert to absolute, then .sum(axis=1) to sum horizontally, to yield a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df.drop(columns=['Amount']).normalize_columns().abs().min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating a homemade outlier score (group assignment, 5 minutes)\n",
    "\n",
    "Generate an array with outlier scores based on your own hand-made logic. Store the outlier predictions in a pandas Series with the name \"homemade_outlier_scores\", using the examples below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** what shape should this array have? (# rows, # columns)\n",
    "\n",
    "\n",
    "Answer: xxxxx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, create an outlier score using the previously shown concepts. \n",
    "\n",
    "It is recommended to drop a column (which one??) before doing so. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some examples to make an outlier score below. Uncomment (remove the \"#\") to execute it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Some options below. Note that only the last executed line will be kept!\n",
    "\n",
    "# homemade_outlier_scores = X['Amount']\n",
    "# homemade_outlier_scores = X['V1'].abs()\n",
    "# homemade_outlier_scores = X.iloc[:, :10].abs().max(axis=1)\n",
    "# homemade_outlier_scores = xxxxx (your own score, if desired)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify the shape, add .shape to the dataframe and look at the output\n",
    "homemade_outlier_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use an outlier algorithm to generate outlier scores (10 minutes)\n",
    "\n",
    "Go to the section of the outlier algorithm assigned to you to generate your scores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "# !pip install seaborn==0.11.1 # Needed for plotting\n",
    "!pip install tensorflow\n",
    "import numpy as np\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahalanobis Distance\n",
    "\n",
    "As the name suggests, the Mahalanobis distance is a distance-based outlier detection. The Mahalonobis distance is a generalization of the distance in standard deviation to any multidimensional, normal distribution.\n",
    "\n",
    "In the cells below: \n",
    "- Create an EmpiricalCovariance object with the correct parameters (find out which ones)\n",
    "- Fit the data to this model\n",
    "- Assign the scores to \"mah_outlier_scores\", using the method \"mahalanobis\"\n",
    "\n",
    "Can the outcome of this method be directly used as an outlier score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?EmpiricalCovariance\n",
    "\n",
    "# cov = EmpiricalCovariance(xxxx)\n",
    "# cov.fit(xxxx)\n",
    "# mah_outlier_scores = cov.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: in which situation is the Mahalonobis-distance equal to do a simple, column-wise mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbours\n",
    "\n",
    "The family ofnearest neighbours-based algorithms looks at the immediate \"neighbour\" in terms of nearest points. \n",
    "If a point is far-removed from its neighbours, it may be considered untypical. The distance may however be determined in several different ways. \n",
    "\n",
    "In the cells below: \n",
    "- Create an EmpiricalCovariance object with the correct parameters (find out which ones)\n",
    "- Fit the data to this model\n",
    "- Assign the scores to \"mah_outlier_scores\", using the method \"mahalonobis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a NearestNeighbors object, and use that. First, we may want to read some documentation regarding the NearestNeighbors class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?NearestNeighbors\n",
    "# nn = NearestNeighbors(xxxxx)\n",
    "# nn.fit(xxxxx)\n",
    "# distances_to_neighbors = nn.kneighbors()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"heavy lifting\" was done by the kneighbors() method. \n",
    "It returns the distances to the first N points, and the index of the nearest point \n",
    "\n",
    "\n",
    "As a final step, reduce the distance matrix (size: N points x N neighbours) to scores, one per point (row). \n",
    "Use an appropiate numpy function (ideas are: mean, median, min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_mean_outlier_scores = np.xxxx(distances_to_neighbors, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: what is an interpretation (say when n_neighbours is 11) for the\n",
    "\n",
    "- median\n",
    "- min\n",
    "- max\n",
    "\n",
    "distance to the n_neighbours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest algorithm\n",
    "\n",
    "The isolation forest algorithm measures how difficult it is to isolate a point from the rest of the data, using a random splitting algorithm many times. We want to have 1000 estimators (trees), and each estimator to be based on a sample of 1024 points\n",
    "\n",
    "In the cells below: \n",
    "- Create an IsolationForest object with the correct parameters\n",
    "- Fit the IsolationForest object with the data\n",
    "- Get the scores using .score_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?IsolationForest\n",
    "# iforest = IsolationForest(xxxx=xxxxx, xxx=xxxxxx)\n",
    "# iforest.fit(xxxx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score as returned by score_samples() is a measure for the number of needed splits to isolate a point. \n",
    "\n",
    "**Question:** Is a high score or a low score an indication for a point being an outlier? \n",
    "Reflect this in your score calculation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iforest_outlier_scores = iforest.score_samples(xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture\n",
    "\n",
    "The Gaussian Mixture assumes the data consists of one or multiple \"blobs\" of clusters with some normal distribution (NB: with a co-variance matrix constrained to be spherical, diagonal or non-constrained - full). \n",
    "After fitting, the method .score_samples() returns some probability measure (probability density of the point within the gaussian mixture distribution). \n",
    "\n",
    "In the cells below, \n",
    "\n",
    "- Create a GaussianMixture object with sensible parameters \n",
    "- Fit the object to the data\n",
    "- Get scores for the individual data points using .score_samples() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?GaussianMixture\n",
    "# gmm = GaussianMixture(xxx=xxxx, xxx=xxx, random_state=1) \n",
    "# gmm.fit(xxxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: The .score_samples method returns a probability. Can the scores be used as they are or do they need to be modified? Reflect this in your calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm_scores = gmm.score_samples(xxx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Autoencoder\n",
    "\n",
    "Autoencoders are a special type of neural networks, that are trained to effectively compress and decompress a signal. The idea behind using these networks for outlier detection, is that the neural network is expected to handle \"typical\" datapoints well, whereas it will struggle with outliers. \n",
    "\n",
    "We use the pyod AutoEncoder class, because this way we don't have to define all details of the neural network architecture, but can specify the main parameters. \n",
    "\n",
    "- Use the scaled data X_scaled (why do you think?)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "clf = AutoEncoder(\n",
    "    hidden_neurons=[10, xxxx, 10], # Choose bottleneck here!\n",
    "    hidden_activation='elu',\n",
    "    output_activation='sigmoid', # Choose an activation ('linear', 'sigmoid', 'relu', 'elu' are some possibilities)\n",
    "    input_activation='sigmoid',\n",
    "    optimizer='adam',\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    dropout_rate=0.0, #may not be needed here\n",
    "    l2_regularizer=0.0,\n",
    "    validation_size=0.1,\n",
    "    preprocessing=False, #NB: this uses sklearn's StandardScaler\n",
    "    verbose=1,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?clf.predict_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Inspect the documentation on .predict_proba, and calculate the outlier scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoenc_outlier_scores = clf.predict_proba(xxxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3: Plot and compare results (5 min)\n",
    "\n",
    "In the next section, you will compare how your algorithm did against your \"home-made\" algorithm, using the labels (ground-truth: is a point an outlier or not? In this case: is a transaction fraudulent or not?) \n",
    "\n",
    "Note that this information is usually not available for those problems where we decide to use outlier detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels, and a helper module\n",
    "!curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/y_unsupervised.csv.zip\n",
    "!curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/outlierutils.py\n",
    "y = pd.read_csv('y_unsupervised.csv.zip')['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from outlierutils import plot_top_N, plot_outlier_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the conditional distributions of the scores, and the AUC metrics\n",
    "\n",
    "(NB: only plot your \"homemade\" score and your own algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log1p(homemade_outlier_scores), title='Homemade: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log1p(knn_mean_outlier_scores), title='KNN: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log1p(iforest_outlier_scores), title='Isolation Forest: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log10(mah_outlier_scores), title='Mahalonobis: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log10(autoenc_outlier_scores), title='Autoencoder: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log10(gmm_scores - np.min(gmm_scores) + 1), title='GMM: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the precision@top-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=homemade_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=knn_mean_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=iforest_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=mah_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=autoenc_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=gmm_scores, N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** based on the number of positives, what precision do you expect when randomly guessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "\n",
    "\n",
    "- How did you construct your home-made outlier model?\n",
    "- How did it perform?\n",
    "- What choices did you make for the outlier algorithm, if any, and why?\n",
    "- How do both compare?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

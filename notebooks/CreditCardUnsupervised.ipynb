{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DonErnesto/masterclassSFI_2021/blob/main/notebooks/CreditCardUnsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud detection with Unsupervised Machine Learning\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "\n",
    "The purpose of this Jupyter notebook is to take you through several unsupervised outlier detection algorithms, and show their value for the purpose of fraud detection. The completely different approach compared to the supervised case, is that we assume we have insufficient labels to predict future patterns, as is often the case in fraud detection. In applying an outlier detection for fraud detection, we hypothesize that payment patterns that are untypical are more likely to be fraudulent. If this hypothesis is true, a compliance function is much more likely to spot fraudulent payments when inspecting a sample of payments that are outliers, compared to a sample that is drawn at random. \n",
    "\n",
    "\n",
    "Generally, whenever an unsupervised approach is chosen, there are no labels available; neither for algorithm optimization, nor for comparison or validation. For this masterclass, we do use a labeled dataset, which will be useful for us to get a feeling of how the various algorithms perform and compare. \n",
    "\n",
    "\n",
    "The data was taken from https://www.kaggle.com/mlg-ulb/creditcardfraud, and downsampled for the purpose of this masterclass. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data import from Github\n",
    "import os\n",
    "if not os.path.exists('X_unsupervised.csv.zip'):\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/X_unsupervised.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the \"pandas\" package for data handling and manipulation, and later \"scikit-learn\" (imported with \"sklearn\") for various outlier detection algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package import: pandas for data handling and manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# A small hack: \"monkey-patching\" the DataFrame class to add column-wise normalization as a method\n",
    "def normalize_columns(self,):\n",
    "    return (self - self.mean()) / self.std()\n",
    "\n",
    "pd.DataFrame.normalize_columns = normalize_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the data in a so-called DataFrame (a pandas object), and inspect it by plotting the N-top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X_unsupervised.csv.zip')\n",
    "# .head() returns a DataFrame, that consists of the first N (default: N=5) rows \n",
    "# of the DataFrame it is applied on\n",
    "X.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data describes credit card transactions, one transaction per row. \n",
    "\n",
    "As you may notice, all features are numeric. All Vx features are the result of a mathematical operation called PCA. In reality, we have to deal often with non-numerical (for instance, categorical data), that requires some effort to make it numerical and suitable for the mathematical models we work with. \n",
    "\n",
    "The pre-fabricated data thus saves us considerable time. \n",
    "\n",
    "Let us first determine the dimensions of the DataFrame (note that the first dimension goes along the rows, the second along columns):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a homemade outlier score\n",
    "\n",
    "Generate an array with outlier scores based on your own hand-made logic. Store the outlier predictions in a pandas Series with the name \"homemade_outlier_scores\", using the examples below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "Below, there are several options to create a hand-crafted outlier score. \n",
    "\n",
    "Which one would you chose, and why? Uncomment your preference, to assign the outcome to the variable 'homemade_outlier_scores'. If you see a better solution, you are free to implement that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some examples to make an outlier score below. Uncomment (remove the \"#\") to execute it.\n",
    "\n",
    "# homemade_outlier_scores = X['V1'].abs()\n",
    "# homemade_outlier_scores = (X.normalize_columns()**2).mean(axis=1)\n",
    "# homemade_outlier_scores = X['Amount']\n",
    "# homemade_outlier_scores = X.drop(columns='Time').abs().max(axis=1) # .drop() returns the cropped dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify the shape, add .shape to the dataframe and look at the output. What shape should it be?\n",
    "homemade_outlier_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use an outlier algorithm to generate outlier scores (10 minutes)\n",
    "\n",
    "Go to the section of the outlier algorithm assigned to you or chosen by you to generate your scores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "# !pip install seaborn==0.11.1 # Needed for plotting\n",
    "# !pip install tensorflow\n",
    "!pip install pyod\n",
    "import numpy as np\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahalanobis Distance\n",
    "\n",
    "As the name suggests, the Mahalanobis distance is a distance-based outlier detection. The Mahalonobis distance is a generalization of the distance in standard deviation to any multidimensional, normal distribution.\n",
    "\n",
    "In the cells below: \n",
    "- Create an EmpiricalCovariance object with the correct parameters (find out which ones)\n",
    "- Fit the data to this model\n",
    "- Assign the scores to \"mah_outlier_scores\", using the method \"mahalanobis\"\n",
    "\n",
    "Can the outcome of this method be directly used as an outlier score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?EmpiricalCovariance\n",
    "\n",
    "# cov = EmpiricalCovariance(xxxx)\n",
    "# cov.fit(xxxx)\n",
    "# mah_outlier_scores = cov.???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: in which situation is the Mahalonobis-distance equal to do a simple, column-wise mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest neighbours\n",
    "\n",
    "The family ofnearest neighbours-based algorithms looks at the immediate \"neighbour\" in terms of nearest points. \n",
    "If a point is far-removed from its neighbours, it may be considered untypical. The distance may however be determined in several different ways. \n",
    "\n",
    "In the cells below: \n",
    "- Create an EmpiricalCovariance object with the correct parameters (find out which ones)\n",
    "- Fit the data to this model\n",
    "- Assign the scores to \"mah_outlier_scores\", using the method \"mahalonobis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a NearestNeighbors object, and use that. First, we may want to read some documentation regarding the NearestNeighbors class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?NearestNeighbors\n",
    "# nn = NearestNeighbors(xxxxx)\n",
    "# nn.fit(xxxxx)\n",
    "# distances_to_neighbors = nn.kneighbors()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"heavy lifting\" was done by the kneighbors() method. \n",
    "It returns the distances to the first N points, and the index of the nearest point \n",
    "\n",
    "\n",
    "As a final step, reduce the distance matrix (size: N points x N neighbours) to scores, one per point (row). \n",
    "Use an appropiate numpy function (ideas are: mean, median, min, max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_mean_outlier_scores = np.xxxx(distances_to_neighbors, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: what is an interpretation (say when n_neighbours is 11) for the\n",
    "\n",
    "- median\n",
    "- min\n",
    "- max\n",
    "\n",
    "distance to the n_neighbours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation Forest algorithm\n",
    "\n",
    "The isolation forest algorithm measures how difficult it is to isolate a point from the rest of the data, using a random splitting algorithm many times. We want to have 1000 estimators (trees), and each estimator to be based on a sample of 1024 points\n",
    "\n",
    "In the cells below: \n",
    "- Create an IsolationForest object with the correct parameters\n",
    "- Fit the IsolationForest object with the data\n",
    "- Get the scores using .score_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?IsolationForest\n",
    "# iforest = IsolationForest(xxxx=xxxxx, xxx=xxxxxx)\n",
    "# iforest.fit(xxxx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score as returned by score_samples() is a measure for the number of needed splits to isolate a point. \n",
    "\n",
    "**Question:** Is a high score or a low score an indication for a point being an outlier? \n",
    "Reflect this in your score calculation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iforest_outlier_scores = iforest.score_samples(xxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture\n",
    "\n",
    "The Gaussian Mixture assumes the data consists of one or multiple \"blobs\" of clusters with some normal distribution (NB: with a co-variance matrix constrained to be spherical, diagonal or non-constrained - full). \n",
    "After fitting, the method .score_samples() returns some probability measure (probability density of the point within the gaussian mixture distribution). \n",
    "\n",
    "In the cells below, \n",
    "\n",
    "- Create a GaussianMixture object with sensible parameters \n",
    "- Fit the object to the data\n",
    "- Get scores for the individual data points using .score_samples() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?GaussianMixture\n",
    "# gmm = GaussianMixture(xxx=xxxx, xxx=xxx, random_state=1) \n",
    "# gmm.fit(xxxx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: The .score_samples method returns a probability. Can the scores be used as they are or do they need to be modified? Reflect this in your calculation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm_scores = gmm.score_samples(xxx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Autoencoder\n",
    "\n",
    "Autoencoders are a special type of neural networks, that are trained to effectively compress and decompress a signal. The idea behind using these networks for outlier detection, is that the neural network is expected to handle \"typical\" datapoints well, whereas it will struggle with outliers. \n",
    "\n",
    "We use the pyod AutoEncoder class, because this way we don't have to define all details of the neural network architecture, but can specify the main parameters. \n",
    "\n",
    "- Use the scaled data X_scaled (why do you think?)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "clf = AutoEncoder(\n",
    "    hidden_neurons=[10, xxxx, 10], # Choose bottleneck here!\n",
    "    hidden_activation='elu',\n",
    "    output_activation='sigmoid', # Choose an activation ('linear', 'sigmoid', 'relu', 'elu' are some possibilities)\n",
    "    optimizer='adam',\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    dropout_rate=0.0, #may not be needed here\n",
    "    l2_regularizer=0.0,\n",
    "    validation_size=0.1,\n",
    "    preprocessing=False, #NB: this uses sklearn's StandardScaler\n",
    "    verbose=1,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?clf.decision_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Inspect the documentation on .decision_scores_, and calculate the outlier scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoenc_outlier_scores = clf.decision_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3: Plot and compare results (5 min)\n",
    "\n",
    "In the next section, you will compare how your algorithm did against your \"home-made\" algorithm, using the labels (ground-truth: is a point an outlier or not? In this case: is a transaction fraudulent or not?). \n",
    "Note that this information is usually not available for those problems where we decide to use outlier detection.\n",
    "\n",
    "\n",
    "Look carefully at the plots and assess their meaning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels, and a helper module\n",
    "!curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/y_unsupervised.csv.zip\n",
    "!curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/ml_utils.py\n",
    "y = pd.read_csv('y_unsupervised.csv.zip')['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils import plot_top_N, plot_outlier_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the conditional distributions of the scores, and the AUC metrics\n",
    "\n",
    "(NB: only plot your \"homemade\" score and your own algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log1p(homemade_outlier_scores), title='Homemade: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log1p(knn_mean_outlier_scores), title='KNN: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log1p(iforest_outlier_scores), title='Isolation Forest: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log10(mah_outlier_scores), title='Mahalonobis: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log10(autoenc_outlier_scores), title='Autoencoder: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y.values, np.log10(gmm_scores - np.min(gmm_scores) + 1), title='GMM: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the precision@top-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=homemade_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=knn_mean_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=iforest_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=mah_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=autoenc_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=gmm_scores, N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** based on the number of positives, what precision do you expect when randomly guessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "\n",
    "\n",
    "- How did you construct your home-made outlier model?\n",
    "- How did it perform?\n",
    "- What choices did you make for the outlier algorithm, if any, and why?\n",
    "- How do both compare?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

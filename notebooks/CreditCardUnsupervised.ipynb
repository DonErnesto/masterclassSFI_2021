{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DonErnesto/masterclassSFI_2021/blob/main/notebooks/CreditCardUnsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Fraud Detection\n",
    "\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "\n",
    "The purpose of this Jupyter notebook is to take you through several unsupervised outlier detection algorithms, and show their value for the purpose of fraud detection. The completely different approach compared to the supervised case, is that we assume we have insufficient labels to recognize fraud patterns, as is often the case in fraud detection (fraud events are often very rare). In applying an outlier detection for fraud detection, we hypothesize that payment patterns that are \"untypical\" (outliers) are more likely to be fraudulent.  \n",
    "\n",
    "\n",
    "Generally, whenever an unsupervised approach is chosen, there are no labels available; neither for algorithm optimization, nor for comparison or validation. For this masterclass, we do use a labeled dataset, which will only be used at the very end, to get a feeling of how the various algorithms perform and compare. \n",
    "\n",
    "\n",
    "The data was taken from https://www.kaggle.com/mlg-ulb/creditcardfraud, and downsampled for the purpose of this masterclass. \n",
    "\n",
    "Note that for each algorithm, we want outliers get higher scores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data import from Github\n",
    "import os\n",
    "if not os.path.exists('X_unsupervised.csv.zip'):\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/X_unsupervised.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the \"pandas\" package for data handling and manipulation, and later \"scikit-learn\" (imported with \"sklearn\") for various outlier detection algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package import: pandas for data handling and manipulation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "\n",
    "# A small hack: \"monkey-patching\" the DataFrame class to add column-wise normalization as a method\n",
    "def normalize_columns(self,):\n",
    "    return (self - self.mean()) / self.std()\n",
    "\n",
    "pd.DataFrame.normalize_columns = normalize_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the data in a so-called DataFrame (a pandas object), and inspect it by plotting the N-top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X_unsupervised.csv.zip')\n",
    "# .head() returns a DataFrame, that consists of the first N (default: N=5) rows \n",
    "# of the DataFrame it is applied on\n",
    "X.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data describes credit card transactions, one transaction per row. \n",
    "\n",
    "As you may notice, all features are numeric. All Vx features are the result of a mathematical operation called PCA. In reality, we have to deal often with non-numerical (for instance, categorical data), that requires some effort to make it numerical and suitable for the mathematical models we work with. \n",
    "\n",
    "The pre-fabricated data thus saves us considerable time. \n",
    "\n",
    "Let us first determine the dimensions of the DataFrame (note that the first dimension goes along the rows, the second along columns):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: Defining a homemade outlier score\n",
    "\n",
    "Generate an array with outlier scores based on your own hand-made logic. Store the outlier predictions in a pandas Series with the name \"homemade_outlier_scores\", using the examples below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "\n",
    "Below, there are several options to create a hand-crafted outlier score. \n",
    "\n",
    "Which one would you chose, and why? Uncomment your preference, to assign the outcome to the variable 'homemade_outlier_scores'. If you see a better solution, you are free to implement that. \n",
    "\n",
    "The various methods that are used on the DataFrame are:\n",
    "- .abs() This method converts all values to their absolute (and does not change the size of the DataFrame)\n",
    "- .drop(columns=...) This method returns the DataFrame with the indicated columns (may be a string or a list of strings) removed\n",
    "- .max(axis=1) This method, when executed with axis=1, sums over all columns\n",
    "- .mean(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some examples to make an outlier score below. Uncomment (remove the \"#\") to execute it.\n",
    "\n",
    "# homemade_outlier_scores = X.drop(columns='Time').abs().max(axis=1)\n",
    "# homemade_outlier_scores = (X.normalize_columns()**2).mean(axis=1)\n",
    "# homemade_outlier_scores = X['Amount']\n",
    "# homemade_outlier_scores = X.drop(columns='Time').abs().max(axis=1) # .drop() returns the cropped dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify the shape, add .shape to the dataframe and look at the output. What shape should it be?\n",
    "# homemade_outlier_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier algorithms\n",
    "\n",
    "Go to the section of the outlier algorithm assigned to you or chosen by you to generate your scores. \n",
    "First run the cell below for important imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "# !pip install seaborn==0.11.1 # Needed for plotting\n",
    "# !pip install tensorflow\n",
    "import numpy as np\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "try:\n",
    "    import pyod\n",
    "except ModuleNotFoundError:\n",
    "    !pip install pyod\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also get rid of the \"Time\" column by executing the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns='Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mahalanobis Distance\n",
    "\n",
    "The Mahalanobis distance is a generalization of distance (measured in standard deviations) to a multivariate normal distribution. The assumption being made is thus that the data is normally distributed, and that outliers are located further away from the center than the inliers. \n",
    "\n",
    "Run the cells below to: \n",
    "- Create an EmpiricalCovariance object\n",
    "- Fit the data to this model\n",
    "- Assign the scores to \"mah_outlier_scores\", using the method \"mahalanobis\"\n",
    "\n",
    "If necessary, change the sign of the assignment from`+` to `-`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?EmpiricalCovariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = EmpiricalCovariance()\n",
    "cov.fit(X)\n",
    "mah_outlier_scores = + cov.mahalanobis(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?cov.mahalanobis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: in which situation is the Mahalonobis-distance equal to do a simple, column-wise mean of squared values? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture\n",
    "\n",
    "The Gaussian Mixture is a assumes the data consists of one or multiple \"blobs\" of clusters with some normal distribution (NB: with a co-variance matrix constrained to be spherical, diagonal or non-constrained - full). It is a \"soft clustering\" method, as each point may belong to each cluster with some probability. \n",
    "After fitting, the method .score_samples() returns some probability measure (probability density of the point within the gaussian mixture distribution). \n",
    "\n",
    "Run the cells below to: \n",
    "\n",
    "- Create a GaussianMixture object (you may adapt the parameters if desired)\n",
    "- Fit the object to the data\n",
    "- Get scores for the individual data points using `.score_samples()`\n",
    "\n",
    "If necessary, change the sign of the assignment from `+` to `-` (read the documentation to decide this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=5, covariance_type='full', random_state=1, n_init=3) \n",
    "gmm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_scores = +gmm.score_samples(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?gmm.score_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?gmm.score_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: It is not really possible to know in advance a good value for the number of components. Can you think of a procedure to estimate it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbours\n",
    "\n",
    "Neighbourhood-based algorithms look at the points in the vicinity of a pointto determine its \"outlierness\". \n",
    "In the most basic NearestNeighbor algorithm as used here, the distance of a point to its neighbours is used to measure its outlierness. (The more involved LOF algorithm uses the deviation in local density of a data point with respect to its neighbors). \n",
    "\n",
    "Run the cells below to: \n",
    "- Create a NearestNeighbors object (adapt the parameters if you wish)\n",
    "- Fit the data to this model\n",
    "- Assign the scores to `knn_outlier_scores`, by aggregating the data in `distances_to_neighbors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a NearestNeighbors object, and use that. First, we may want to read some documentation regarding the NearestNeighbors class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NearestNeighbors(n_neighbors=50)\n",
    "nn.fit(X)\n",
    "distances_to_neighbors = nn.kneighbors()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"heavy lifting\" was done by the `.kneighbors()` method. \n",
    "It returns for each point the distances to the nearest N points, and the index of the nearest point. \n",
    "\n",
    "As a final step, we collapse this distance matrix (m points x N neighbours) to m scores. This may be done in several ways, for instance by taking the mean, or the median. Choose one of the options given below (by default, the mean is taken). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_outlier_scores = np.mean(distances_to_neighbors, axis=1)\n",
    "# knn_outlier_scores = np.median(distances_to_neighbors, axis=1)\n",
    "# knn_outlier_scores = np.max(distances_to_neighbors, axis=1)\n",
    "# knn_outlier_scores = np.min(distances_to_neighbors, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: what is an interpretation (say when n_neighbours is 11) for the\n",
    "\n",
    "- median\n",
    "- min\n",
    "- max\n",
    "\n",
    "distance to the n_neighbours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest algorithm\n",
    "\n",
    "The isolation forest algorithm measures how difficult it is to isolate a point from the rest of the data, using a random splitting algorithm. Similar to the Random Forest algorithm, many (`n_estimators`) different trees are built, each time based on a randomly drawn sample (of size `max_samples`).\n",
    "\n",
    "Run the cells below to: \n",
    "- Create an IsolationForest object with the correct parameters\n",
    "- Fit the IsolationForest object with the data\n",
    "- Get the scores using `.score_samples()`\n",
    "\n",
    "If necessary, change the sign of the assignment from `+` to `-` (read the documentation, and note that the score as returned by `.score_samples()` is a measure for the number of needed splits to isolate a point). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest = IsolationForest(n_estimators=100, max_samples=1024)\n",
    "iforest.fit(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest_outlier_scores = +iforest.score_samples(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?iforest.score_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What advantage and disadvantage of this method do you see?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Autoencoder\n",
    "\n",
    "Autoencoders are a special type of neural networks, that are trained to effectively compress and decompress a signal. The idea behind using these networks for outlier detection, is that the neural network is expected to handle \"typical\" datapoints well, whereas it will struggle with outliers. \n",
    "\n",
    "We use the pyod `AutoEncoder` class to construct the network. This way we don't have to bother with the details of building the network, and can focus on the main parameters. \n",
    "\n",
    "Run the cells below to: \n",
    "- Create an Autoencoder object\n",
    "- Train this object on the data\n",
    "- Get the scores using .score_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = MinMaxScaler().fit_transform(X)\n",
    "clf = AutoEncoder(\n",
    "    hidden_neurons=[10, 5, 10], # Choose bottleneck here!\n",
    "    hidden_activation='elu',\n",
    "    output_activation='sigmoid', \n",
    "    optimizer='adam',\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    dropout_rate=0.0, #may not be needed here\n",
    "    l2_regularizer=0.0,\n",
    "    validation_size=0.1,\n",
    "    preprocessing=False, #NB: this uses sklearn's StandardScaler\n",
    "    verbose=1,\n",
    "    random_state=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoenc_outlier_scores = + clf.decision_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Why do you think we had to scale the data with a MinMaxScaler (hint: the signal needs to be reconstructed by the network). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and compare results\n",
    "\n",
    "In the next section, you will compare how your algorithm did against your \"home-made\" algorithm, using the labels (ground-truth: is a point an outlier or not? In this case: is a transaction fraudulent or not?). \n",
    "Note that this information is usually not available for those problems where we decide to use outlier detection.\n",
    "\n",
    "\n",
    "Look carefully at the plots and assess their meaning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels, and a helper module\n",
    "force_download = False\n",
    "if force_download or not os.path.exists('y_unsupervised.csv.zip'): # then probably nothing was downloaded yet\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/y_unsupervised.csv.zip\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/ml_utils.py\n",
    "y = pd.read_csv('y_unsupervised.csv.zip')['Class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils import plot_top_N, plot_conditional_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional distributions of the scores, and the AUC metrics\n",
    "\n",
    "NB: \n",
    "- plot your \"homemade\" score and your own algorithm\n",
    "- use np.log1p(your_score) to logtransform your score if needed (in case it has a very long \"tail\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plot_outlier_scores(y.values, np.log1p(homemade_outlier_scores), title='Homemade: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_conditional_distribution(y.values, knn_mean_outlier_scores), title='KNN: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_conditional_distribution(y.values, iforest_outlier_scores, title='Isolation Forest: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_conditional_distribution(y.values, mah_outlier_scores, title='Mahalonobis: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_conditional_distribution(y.values, autoenc_outlier_scores, title='Autoencoder: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_conditional_distribution(y.values, (gmm_scores - np.min(gmm_scores)), title='GMM: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots shows how many of the top-N points in terms of score were actual outliers (the more yellow the plot, the better the algorithm performed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision@top-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plot_top_N(y_true=y, scores=homemade_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=knn_mean_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=iforest_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=mah_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=autoenc_outlier_scores, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_true=y, scores=gmm_scores, N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** based on the number of positives, what precision do you expect when randomly guessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "\n",
    "\n",
    "- How did you construct your home-made outlier model?\n",
    "- How did it perform?\n",
    "- What choices did you make for the outlier algorithm, if any, and why?\n",
    "- Answers to the questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DonErnesto/amld2020-unsupervised/blob/master/notebooks/exercises_1_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ek6rUiPXRC-"
   },
   "source": [
    "## Exercise: Demonstration of several algorithms on the Pen Digits dataset\n",
    "\n",
    "Exercises using the Pen Digits Dataset: https://www.dbs.ifi.lmu.de/research/outlier-evaluation/DAMI/literature/PenDigits/PenDigits_v01.html\n",
    "\n",
    "The data is already present in .pkl format (load the data below).\n",
    "This data set was created by recording the writing pattern of digits on a digital writing pad. The digit \"4\" is downsampled to only 20 instances (instead of ~1000 for the other points), making it an outlier. \n",
    "\n",
    "Note that (unlike MNIST), the features do not simply correspond to pixels, but are subsampled coordinates, 8 pairs. \n",
    "\n",
    "This dataset is small and simple: it has only numeric features and no NaN's. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsyJ1d24Z9Je"
   },
   "source": [
    "# Package installing and data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "brdJ1IWkc2UK",
    "outputId": "24ea451a-434e-470c-9e6b-cdb94920df04"
   },
   "outputs": [],
   "source": [
    "# Now only load the required files...\n",
    "!curl -O https://raw.githubusercontent.com/amld/workshop-unsupervised-fraud/master/outlierutils.py\n",
    "!curl -O https://raw.githubusercontent.com/amld/workshop-unsupervised-fraud/master/data/x_pendigits.pkl\n",
    "!curl -O https://raw.githubusercontent.com/amld/workshop-unsupervised-fraud/master/data/y_pendigits.pkl\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "eI3sCkfBZoXX",
    "outputId": "3f2178ef-e537-4d64-d043-7312dbfdf49c"
   },
   "outputs": [],
   "source": [
    "# Install pyod package\n",
    "!pip install pyod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpkjzX_2XRDD"
   },
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_MZmQrSTXRDF",
    "outputId": "ef404de3-cb2b-4f31-88cb-2b88935efb6e"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 1.x\n",
    "# standard library imports\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "# pandas, seaborn etc.\n",
    "import seaborn as sns\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn outlier models\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# other sklearn functions\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import MinCovDet, EmpiricalCovariance\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import scale as preproc_scale\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# pyod\n",
    "import pyod\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.pca import PCA as pyod_PCA\n",
    "from pyod.models.iforest import IForest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hDdOJyUvZaeZ"
   },
   "outputs": [],
   "source": [
    "from outlierutils import plot_top_N, plot_outlier_scores # For easy plotting and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9DYhZQRXRDQ"
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZWbReXmXRDS"
   },
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "x_pen = pd.read_pickle(os.path.join(data_path, 'x_pendigits.pkl'))\n",
    "y_pen = pd.read_pickle(os.path.join(data_path, 'y_pendigits.pkl'))\n",
    "\n",
    "# Scale and put again into a DataFrame\n",
    "sc = StandardScaler()\n",
    "x_pen = pd.DataFrame(data=sc.fit_transform(x_pen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "I7S-ifKHXRDW",
    "outputId": "573ebe03-58dd-4317-948d-d4e345b3571d"
   },
   "outputs": [],
   "source": [
    "print('Number of points: {}'.format(len(y_pen)))\n",
    "print('Number of positives: {} ({:.3%})'.format(y_pen.sum(), y_pen.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Usage of plotting functions\n",
    "\n",
    "See examples how to plot the conditional scores and the top-N ranking below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example with random data\n",
    "y_true_, scores_ = np.random.choice([0, 1], 100), np.random.uniform(size=100)\n",
    "results = plot_outlier_scores(y_true=y_true_, \n",
    "                            scores=scores_, \n",
    "                            bw=0.05, \n",
    "                            title='Example.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next plot shows the true labels of the N points with the highest outlier scores.\n",
    "# More yellow is better!\n",
    "\n",
    "results = plot_top_N(y_true=y_true_, scores=scores_, N=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both `plot_outlier_scores` and `plot_top_N` expect numpy arrays. These may be obtained from a pandas Series using `pd.Series.values`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization\n",
    "\n",
    "t-SNE of large datasets may take a long time to compute. The next piece of code will downsample the negatives, while retaining all positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_downsample = 3000\n",
    "assert x_pen.index.equals(y_pen.index), 'Error, indexes differ. Reset them to continue'\n",
    "x_downsampled = pd.concat((x_pen[y_pen==0].sample(N_downsample - int(y_pen.sum()), random_state=1),\n",
    "                           x_pen[y_pen==1]), \n",
    "                          axis=0).sample(frac=1, random_state=1)\n",
    "y_downsampled = y_pen[x_downsampled.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 0. \n",
    "Reduce the dimensionality with T-SNE, and visualize the positive and negative class in a scatter plot. \n",
    "What do you observe?\n",
    "\n",
    "**Hint**: To get help for a function or class, run `?<object>`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_N_TSNE = 4000 #Avoid overly long computation times with TSNE. Values < 4000 recommended \n",
    "neg = y_downsampled == 0\n",
    "pos = y_downsampled == 1\n",
    "\n",
    "assert len(x_downsampled) <= MAX_N_TSNE, 'Using a dataset with more than {} points is not recommended'.format(\n",
    "                                            MAX_N_TSNE)\n",
    "#X_2D = TSNE(xxxx).fit_transform(x_downsampled) # transform to 2-D space for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "#ax.scatter(X_2D[pos, 0], X_2D[pos, 1], c=[[0.8, 0.4, 0.4],], marker='x', s=120, label='Positive')\n",
    "#ax.scatter(X_2D[neg, 0], X_2D[neg, 1], c=[[0.2, 0.3, 0.9],], marker='o', s=10, label='Negative')\n",
    "\n",
    "#plt.axis('off')\n",
    "#plt.legend()\n",
    "#plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_downsampled, y_downsampled # To avoid using the wrong data later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mahalanobis Distance\n",
    "\n",
    "Using `EmpiricalCovariance`, or `MinCovDet` (a robust estimator), do a `.fit()` to fit the covariance matrix. \n",
    "Determine the distance with `.mahalanobis()` and use this as outlier score. Get the Area Under the ROC-Curve, PR-curve and Precision@100 using `plot_outlier_scores` and `plot_top_N`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov_ = EmpiricalCovariance().fit(x_pen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM\n",
    "\n",
    "Using `GaussianMixture`, with a reasonable value for n_components and `covariance_type=full`, do a `.fit()` and `.score_samples()` to get the log-probability of each sample. The negative log-probability will be the outlier score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm = GaussianMixture(n_components=xxxx, covariance_type='full', random_state=1) # try also spherical\n",
    "# gmm.fit(x_pen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1. \n",
    "Which algorithm performed better, GMM or Mahalonobis? Why do you think?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 2.\n",
    "With the scikit-learn NearestNeighbors class, determine the probability of the nearest neighbour of a point being an outlier, conditional on the class membership of that point. Note how this corresponds to the observations from the t-SNE visualization\n",
    " \n",
    "**HINT**: \n",
    "- *The output of clf.kneighbors() (after doing clf.fit()) is a tuple, the first element being an array with distances, the second element being an array with indexes of neighbours. The first column corresponds to the points themselves. The code provided in the next cell may be used to get the indices of the nearest neighbour of all points.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_nn = NearestNeighbors(n_neighbors=10)\n",
    "# clf_nn.fit(x_pen)\n",
    "# nearest_1st_n = clf_nn.kneighbors(x_pen)[1][:, 1] # to get the indices of the first nearest neighbour for each point\n",
    "\n",
    "## Next, get the conditional mean by subsetting nearest_1st_ns with y_pen==1 or y_pen==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pyod's KNN class to detect the outliers. \n",
    "Use `method=median`, and guess a reasonable value for `n_neighbors`based on the insights from t-SNE and the previous question. \n",
    "\n",
    "Plot the conditional score curves and the top-100 results, using `plot_outlier_scores` and `plot_top_N`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 3.\n",
    "\n",
    "\n",
    "\n",
    "Vary n_neighbors, how does it affect AUC-ROC, AUC-PR and precision@100?\n",
    "\n",
    "**HINT**: \n",
    "- *Use clf.decision_scores_ (an attribute of all pyod's detectors) to get the outlier scores (and not the binary labels, that are stored in clf.labels_)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = KNN(method='median', n_neighbors=xxx)\n",
    "# clf.fit()\n",
    "# get the prediction label and outlier scores of the training data\n",
    "# y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "# y_train_scores = clf.decision_scores_  # raw outlier scores (use these for scoring!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOF \n",
    "\n",
    "Note that the LOF algorithm compares the \"reachability density\" of a point to its k nearest neighbours, compared to that same density of its nearest neighbours.\n",
    "\n",
    "Also here, we will use a pyod class. Therefore, we can use the same method- and attribute names. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 4.\n",
    "Considering the concept that underlies LOF, and the results from the t-SNE/Nearest neighbour analysis, do you expect LOF to do better than KNN with n_neighbours=10? Why?\n",
    "\n",
    "#### Q 5.\n",
    "Plot the scoring curves for a few values of n_neighbours. What is a good value?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lof = LOF(n_neighbors=n_neighbours, contamination=0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 6. \n",
    "What disadvantage of Isolation Forest do you see compared to the previous algorithms?\n",
    "\n",
    "Run an `IsolationForest`analysis with a reasonable set of parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ifo = IForest(behaviour='new',n_estimators=10, max_samples=512)\n",
    "#ifo.fit(xxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (optional question)\n",
    "Compare the scatter in AUC scores by running ten times the Isolation Forest, with different `random_state`s, for `n_estimators`=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 7.\n",
    "What number of components would you estimate to be suffificient? How may it be determined?\n",
    "\n",
    "#### Q 8.\n",
    "Determine the Euclidean reconstruction error, by first transforming the data, and then applying the inverse transform. What scores do you get?\n",
    "\n",
    "**Use the sci-kit learn implementation, rather than pyod's PCA class. This one seems not to be implemented well**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=...)\n",
    "# pca_tf = ... \n",
    "# x_pen_recon = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_recon = ((x_pen - x_pen_recon)**2).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder reconstruction error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the autoencoder with a bottleneck size that worked well for PCA. Run for ~10-15 epochs and look at AUC score. \n",
    "Many different configurations (number of hidden layers, number of neurons) may be used, but pick one. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 9. \n",
    "What output activation do you think will work best? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AutoEncoder(\n",
    "    hidden_neurons=[xx, xx, xx], # Choose size here!\n",
    "    hidden_activation='elu',\n",
    "    output_activation='xx', # Choose an activation ('linear', 'sigmoid', 'relu', 'elu' are some possibilities)\n",
    "    optimizer='adam',\n",
    "    epochs=15,\n",
    "    batch_size=16,\n",
    "    dropout_rate=0.0, #may not be needed here\n",
    "    l2_regularizer=0.0,\n",
    "    validation_size=0.1,\n",
    "    preprocessing=False, #NB: this uses sklearn's StandardScaler\n",
    "    verbose=1,\n",
    "    random_state=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(x_pen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 10.\n",
    "\n",
    "- Which algorithm performed best?\n",
    "- Can it be reasonably \"tuned\" without having the labels available?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional, if time permits)\n",
    "\n",
    "Run the next code, that replaces the original outliers with sythesized ones (combining the first 8 and last 8 features of randomly chosen observations), and run the analysis again. What differences do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_values = pd.concat((x_pen.sample(20, random_state=1).iloc[:, :8].reset_index(drop=True),\n",
    "                        x_pen.sample(20, random_state=2).iloc[:, 8:].reset_index(drop=True)),\n",
    "                 axis=1).set_index(y_pen[y_pen==1].index)\n",
    "x_pen.loc[y_pen==1, :] = new_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvz0pAPzXRFa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "exercises_1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

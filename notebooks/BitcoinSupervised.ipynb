{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DonErnesto/masterclassSFI_2021/blob/main/notebooks/BitcoinSupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime detection with Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "\n",
    "The purpose of this Jupyter notebook is to guide you through some essential ingredients when developing a machine learning model: hyperparameter tuning, model comparison and selection. \n",
    "\n",
    "The data we will be using was taken from Kaggle: https://www.kaggle.com/ellipticco/elliptic-data-set \n",
    "and describes blockchain transactions, some of which are flagged as \"illicit\" (i.e., relating to illegal activity), others as \"licit\" or \"unknown\" (the majority, about 80%). We got rid of the unknown labels for simplicity. The authors give as examples of illicit categories: \"scams, malware, terrorist organizations, ransomware, Ponzi schemes, etc.\"\n",
    "\n",
    "\n",
    "Note that there are two types of cells in this notebook: Markdown cells (that contain text, like this one), and Code cells (that execute some code, like the next cell). \n",
    "\n",
    "By clicking the Play button on a cell, we execute a code cell. Lines that start with a \"#\" are comments, and not executed. \n",
    "\n",
    "Your input is required whenever there is a Question (in that case: write in the Markdown cell) or whenever you find some 'xxxxx' in the code cell (in this case, some code needs to be fixed or completed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading the data we will be training on, which has already been splitted into \"X\" (features) and \"y\" (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data import from Github\n",
    "import os\n",
    "if not os.path.exists('X_train_supervised.csv.zip'): # then probably nothing was downloaded yet\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/ml_utils.py\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/X_train_supervised.csv.zip\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/y_train_supervised.csv.zip\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/X_test_supervised.csv.zip\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/y_test_supervised.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using pandas for data handling, and scikit-learn (sklearn) for supervised machine learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package import: pandas for data handling and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ml_utils import slice_gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the data in a so-called DataFrame (a pandas object), and inspect it by plotting the N-top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_supervised.csv.zip')\n",
    "X_test = pd.read_csv('X_test_supervised.csv.zip')\n",
    "y_train = pd.read_csv('y_train_supervised.csv.zip')['class']\n",
    "# .head() returns the first n (per default 5) rows of a DataFrame\n",
    "X_train.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted feature txId\n",
    "X_train = X_train.drop(columns=['txId', 'Time step'])\n",
    "X_test = X_test.drop(columns=['txId', 'Time step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further documentation on this dataset:**\n",
    "\n",
    "From the website: \"There are 166 features associated with each node. Due to intellectual property issues, we cannot provide an exact description of all the features in the dataset. There is a time step associated to each node, representing a measure of the time when a transaction was broadcasted to the Bitcoin network. The time steps, running from 1 to 49, are evenly spaced with an interval of about two weeks. Each time step contains a single connected component of transactions that appeared on the blockchain within less than three hours between each other; there are no edges connecting the different time steps.\n",
    "\n",
    "The first 94 features represent local information about the transaction â€“ including the time step described above, number of inputs/outputs, transaction fee, output volume and aggregated figures such as average BTC received (spent) by the inputs/outputs and average number of incoming (outgoing) transactions associated with the inputs/outputs. The remaining 72 features are aggregated features, obtained using transaction information one-hop backward/forward from the center node - giving the maximum, minimum, standard deviation and correlation coefficients of the neighbour transactions for the same information data (number of inputs/outputs, transaction fee, etc.).\"\n",
    "\n",
    "We only look at the node data (i.e., ignore the network topology), although many of the features are derived from the surrounding nodes and do therefore contain information regarding the network structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, '\\n')\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 33.4k data points, of which 11% is a positive (which is quite a large fraction in a financial crime context). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import the classes we want to use \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we instantiate the DecisionTreeClassifier and define the parameter space we want to explore\n",
    "dtc = DecisionTreeClassifier() #Initialize with whatever parameters you want to\n",
    "\n",
    "# we will vary the maximum depth of the tree, and the minimum required number of samples to make a split\n",
    "param_grid = {'max_depth': [2, 5, 10, 20], 'min_samples_split': [2, 10]} #Note the dictionary notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make use of the GridSearchCV estimator that does the parameter-space scanning for us\n",
    "grid_dtc = GridSearchCV(dtc, param_grid, cv=5, scoring='roc_auc') #NB: uses StratifiedKFold when cv=int\n",
    "\n",
    "# Finally, we fit the GridSearchCV estimator to our training data, using the .fit() method\n",
    "_ = grid_dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?slice_gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use slice_gridsearch (a self-made helper function) to show how the varied parameters influence  classifier performance\n",
    "Note that the boxplots show:\n",
    "- The median, \n",
    "- a box spanning the first and third quartile, \n",
    "- and whiskers that extend to the median +/- 1.5 InterQuartile Range (IQR) or the lowest/highest point. Points beyond the median +/- 1.5 IQR are considered outliers and plotted explicitly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = slice_gridsearch(grid_dtc, vary_parameter_name='max_depth', fix_parameter_name='min_samples_split',\n",
    "                     fix_parameter_value=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection of the returned DataFrame shows us some more interesting statistics\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = slice_gridsearch(grid_dtc, vary_parameter_name='min_samples_split', fix_parameter_name='max_depth',\n",
    "                     fix_parameter_value=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the simple Decision Tree Classifier, ROC-AUC close to 0.90 are feasible. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n",
    "\n",
    "The Naive Bayes classifier is a rather simple and powerful classifier, that has been used successfully in for instance spam filters. Here we will use a classifier that assumes a Gaussian distribution of its features, the Gaussian Naive Bayes classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "param_grid = {'var_smoothing': [1.E-9, 1.E-6, 1.E-3, 1., 100.]} #Note the dictionary notation\n",
    "nb = GaussianNB()\n",
    "sc = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make use of the GridSearchCV estimator that does the parameter-space scanning for us\n",
    "grid_nb = GridSearchCV(nb, param_grid, cv=5, scoring='roc_auc') #NB: uses StratifiedKFold when cv=int\n",
    "\n",
    "# Finally, we fit the GridSearchCV estimator to our training data, using the .fit() method\n",
    "_ = grid_nb.fit(sc.fit_transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_nb.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is the classification-counterpart of Linear Least Squares for regression. Similar to linear regression, we can impose a penalty on larger coefficient values to prevent overfitting. This is called regularization. \n",
    "\n",
    "Too large a penalty (small C-value in the sklearn model) will lead to stable but sub-optimal performance (underfitting), too small a penalty may result in overfitting, especially when the number of features (columns) is high and the number of samples is low. When doing logistic regression, it is important to determine the optimal regularization strength in a cross-validation cycle. \n",
    "\n",
    "It is important that we scale the data before fitting the model when doing regularization (why?). The correct way is to make scaling a part of a cross-validation pipeline, which is done using the Pipeline class of sklearn. \n",
    "The Pipeline object will behave just as a single classifier, having .fit() and .predict() methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(solver='saga', n_jobs=-1, random_state=10))\n",
    "])\n",
    "# define the parameter grid, preceding the argument name with \"lr__\" when it applies to the LogisticRegression\n",
    "param_grid = {'lr__C': np.logspace(-5, 3, num=5), \n",
    "              #'lr__penalty': ['l1', 'l2']\n",
    "             } \n",
    "grid_lr = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = slice_gridsearch(grid_lr, vary_parameter_name='lr__C', fix_parameter_name=None,\n",
    "                     fix_parameter_value=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifiers typically perform quite well over a wide range of parameters. The main parameter to tune is the depth of the individual trees ('max_depth'), which determines the model complexity. Typically, when the number of trees ('n_estimators') is chosen large enough (say, 100 or more), Random Forest classifiers do not easily overfit. This is because the classifier is an ensemble of many tree classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary with the parameters you want to scan. \n",
    "param_grid = {'max_depth': [2, 5, 10, 20], 'n_estimators':[10, 100]}\n",
    "rfc = RandomForestClassifier() #Initialize with whatever parameters you want to\n",
    "\n",
    "grid_rfc = GridSearchCV(rfc, param_grid, cv=5, scoring='roc_auc') #NB: uses StratifiedKFold when cv=int\n",
    "_ = grid_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(grid_rfc.cv_results_)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Slice\" the results (fix one parameter, vary another one)\n",
    "df = slice_gridsearch(grid_rfc, vary_parameter_name='n_estimators', fix_parameter_name='max_depth',\n",
    "                     fix_parameter_value=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Slice\" the results (fix one parameter, vary another one)\n",
    "df = slice_gridsearch(grid_rfc, vary_parameter_name='max_depth', fix_parameter_name='n_estimators',\n",
    "                     fix_parameter_value=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted Trees\n",
    "\n",
    "Gradient boosted trees share some similarities with Random Forests, in that they are an ensemble of trees. Whereas a Random Forest classifier consists of trees grown individually, Gradient Boosting generates trees that successively address misclassifications of the previous trees. Although scikit-learn does have a Gradient Boosting implementation it is advised to use LightGBM, one of the most performant implementations in terms of speed and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm\n",
    "from sklearn.ensemble import GradientBoostingClassifier #scikit-learn implementation. Not advised\n",
    "from lightgbm import LGBMClassifier #roughly 2 orders of magnitude times faster than scikit-learn's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = LGBMClassifier()\n",
    "#clf_gb = GradientBoostingClassifier()\n",
    "param_grid = {\n",
    "    #'max_depth':[2, 5, 10], # sklearn and lightgbm implementation\n",
    "    'num_leaves': [15, 30, 50], # lightgbm implementation\n",
    "    'num_iterations': [20, 50, 100], # lightgbm implementation\n",
    "    #'boosting_type': ['gbdt', 'dart', 'goss'], # lightgbm implementation\n",
    "    }\n",
    "\n",
    "grid_gb = GridSearchCV(estimator=clf_gb, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = slice_gridsearch(grid_gb, vary_parameter_name='num_leaves', fix_parameter_name='num_iterations',\n",
    "                     fix_parameter_value=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = slice_gridsearch(grid_gb, vary_parameter_name='num_iterations', fix_parameter_name='num_leaves',\n",
    "                     fix_parameter_value=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network\n",
    "\n",
    "The feedforward neural network is consists of consecutive layers of neurons. Their weights and biases need to be trained with the training data. We make use of Tensorflow for speed of calculation, and keras as a wrapper around Tensorflow to make the construction of the neural network easier. We will not use dropout, although this is generally recommended as a regularization measure. If desired, feel free to add a dropout layer. \n",
    "\n",
    "Neural networks typically have a lot of parameters that can be tuned: the number of layers, the width of the layers, the activation functions to be used, the batch size, the optimization function. It is advised to only scan for the width of the first two layers and the batch size given the short time available. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf(width):\n",
    "    ann = keras.models.Sequential()\n",
    "    ann.add(keras.layers.Dense(units=width, activation='relu'))\n",
    "    ann.add(keras.layers.Dense(units=width, activation='relu'))\n",
    "    ann.add(keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "    ann.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                metrics=['accuracy', keras.metrics.AUC()])\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=build_clf, epochs=2)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ann', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'ann__batch_size':[8, 16], \n",
    "        'ann__width':[4, 10],\n",
    "        }\n",
    "gs_nn = GridSearchCV(estimator=pipeline, param_grid=params, cv=2)\n",
    "# now fit the dataset to the GridSearchCV object. \n",
    "_ = gs_nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = slice_gridsearch(gs_nn, vary_parameter_name='ann__batch_size', fix_parameter_name='ann__width',\n",
    "                     fix_parameter_value=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = slice_gridsearch(gs_nn, vary_parameter_name='ann__width', fix_parameter_name='ann__batch_size',\n",
    "                     fix_parameter_value=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Having optimized the hyperparameters of our chosen classifier in a cross-validation, we will use this classifier to generate our predictions. \n",
    "\n",
    "The most straightforward option is to use .best_estimator() to access the best performing classifier \n",
    "according to the cross-validation. Per default (as determined by the `refit` argument to the gridsearch object) the entire training data is used to fit this best estimator. \n",
    "\n",
    "We will use the method .predict_proba() to generate scores (that may be interpreted as probabilities) on the test data. We access the probabilities of the class being 1 (True) with the \"[:, 1]\" operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from ml_utils import plot_outlier_scores, plot_top_N # module with helper functions\n",
    "y_test = pd.read_csv('y_test_supervised.csv.zip')['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test data (X_test) with our selected and optimized classifier \n",
    "# We will use the Gridsearch object that has the DecisionTreeClassifier, grid_dtc\n",
    "# Replace this object with yours\n",
    "\n",
    "y_pred_dtc = grid_dtc.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "print(f'The ROC-AUC test score: {roc_auc_score(y_test, y_pred_dtc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?plot_outlier_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_outlier_scores(y_test, y_pred_dtc, bw=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = plot_top_N(y_test, y_pred_dtc, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_lr.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "_ = plot_outlier_scores(y_test, y_pred, bw=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_rfc.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "_ = plot_outlier_scores(y_test, y_pred, bw=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = grid_gb.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "_ = plot_outlier_scores(y_test, y_pred, bw=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs_nn.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "_ = plot_outlier_scores(y_test, y_pred, bw=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

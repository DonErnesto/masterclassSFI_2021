{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DonErnesto/masterclassSFI_2021/blob/main/notebooks/BitcoinSupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anti-Money Laundering with Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "\n",
    "The purpose of this Jupyter notebook is to make you familiar with several common supervised ML models, and to guide you through some essential steps when developing an ML model: hyperparameter tuning, model comparison and selection. \n",
    "\n",
    "The data we will be using was taken from Kaggle: https://www.kaggle.com/ellipticco/elliptic-data-set \n",
    "and describes blockchain transactions, some of which are flagged as \"illicit\" (i.e., relating to illegal activity), others as \"licit\" or \"unknown\" (the majority, about 80%). We got rid of the unknown labels for simplicity. The authors give as examples of illicit categories: \"scams, malware, terrorist organizations, ransomware, Ponzi schemes, etc.\"\n",
    "\n",
    "\n",
    "Note that there are two types of cells in this notebook: Markdown cells (that contain text, like this one), and Code cells (that execute some code, like the next cell). \n",
    "\n",
    "By clicking the Play button on a cell, we execute a code cell. Lines that start with a \"#\" are comments, and not executed. \n",
    "\n",
    "Your input is required whenever there is a Question (in that case: write in the Markdown cell) or whenever you find some 'xxxxx' in the code cell (in this case, some code needs to be fixed or completed).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by downloading the data we will be training on, which has already been splitted into \"X\" (features) and \"y\" (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data import from Github\n",
    "import os\n",
    "if False or not os.path.exists('X_train_supervised.csv.zip'): # then probably nothing was downloaded yet\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/ml_utils.py\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/X_train_supervised.csv.zip\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/y_train_supervised.csv.zip\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/X_test_supervised.csv.zip\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/y_test_supervised.csv.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using pandas for data handling, and scikit-learn (sklearn) for supervised machine learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Package import: pandas for data handling and manipulation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ml_utils import grouped_boxplot_gridsearch, plot_conditional_distribution\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve, plot_precision_recall_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the data in a so-called DataFrame (a pandas object), and inspect it by plotting the N-top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_supervised.csv.zip')\n",
    "X_test = pd.read_csv('X_test_supervised.csv.zip')\n",
    "y_train = pd.read_csv('y_train_supervised.csv.zip')['class']\n",
    "# .head() returns the first n (per default 5) rows of a DataFrame\n",
    "X_train.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted feature txId\n",
    "X_train = X_train.drop(columns=['txId', 'Time step'])\n",
    "X_test = X_test.drop(columns=['txId', 'Time step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further documentation on this dataset:**\n",
    "\n",
    "From the Kaggle-website ( https://www.kaggle.com/ellipticco/elliptic-data-set ): \"There are 166 features associated with each node. Due to intellectual property issues, we cannot provide an exact description of all the features in the dataset. There is a time step associated to each node, representing a measure of the time when a transaction was broadcasted to the Bitcoin network. ...\n",
    "\n",
    "The first 94 features represent local information about the transaction â€“ including the time step described above, number of inputs/outputs, transaction fee, output volume and aggregated figures such as average BTC received (spent) by the inputs/outputs and average number of incoming (outgoing) transactions associated with the inputs/outputs. The remaining 72 features are aggregated features, obtained using transaction information one-hop backward/forward from the center node - giving the maximum, minimum, standard deviation and correlation coefficients of the neighbour transactions for the same information data (number of inputs/outputs, transaction fee, etc.).\"\n",
    "\n",
    "Finally, and perhaps most relevant, the labels have been generated by a \"heuristics-based reasoning process\", as can be read in the article by Weber et al. (https://arxiv.org/pdf/1908.02591.pdf):\n",
    "\n",
    "\"For example, a higher number of inputs and the reuse of the same address is commonly associated with higher address-clustering, which results in a degrade of anonymity for the entity signing the transaction. On the other hand, consolidating funds controlled by multiple addresses in one single transaction provides benefits in terms of transaction costs (fee). It follows that entities eschewing anonymity-preserving measures for large volumes of user requests are likely to be licit (e.g. exchanges). In contrast, illicit activity may tend to favor transactions with a lower number of inputs to reduce\". \n",
    "\n",
    "This essentially means that the labels were generated by hand-crafted rules. It is thus possible, that these rules may be reconstructed by the model through proxy-features, and that a higher-than-realistic score is achieved. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, '\\n')\n",
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 33.4k data points, of which 11% is a positive (which is quite a large fraction in a financial crime context). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import the classes we want to use \n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we instantiate the DecisionTreeClassifier and define the parameter space we want to explore\n",
    "dtc = DecisionTreeClassifier(random_state=3) #Initialize with whatever parameters you want to\n",
    "\n",
    "# we will vary the maximum depth of the tree, and the minimum required number of samples to make a split\n",
    "param_grid = {'max_depth': [2, 5, 10, 20]} #Note the dictionary notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make use of the GridSearchCV estimator that does the scanning of the hyper-parameters for us,\n",
    "# in a k-Fold cross-validation loop\n",
    "grid_dtc = GridSearchCV(dtc, param_grid, cv=10, scoring='roc_auc', \n",
    "                        return_train_score=True) #NB: uses StratifiedKFold when cv=int\n",
    "\n",
    "# Finally, we fit the GridSearchCV estimator to our training data, using the .fit() method\n",
    "_ = grid_dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use grouped_boxplot_gridsearch (a self-made helper function) to show how classifier performance is affected by the various hyperparameter settings. \n",
    "\n",
    "Note that the boxplots show:\n",
    "- The median (\"mid-point\"), \n",
    "- a box spanning the first and third quartile, \n",
    "- and whiskers that extend to the median +/- 1.5 InterQuartile Range (IQR) or the lowest/highest point. Points beyond the median +/- 1.5 IQR are considered outliers and plotted explicitly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_dtc, parameter_names=['max_depth'])\n",
    "_ = plt.title('CV test scores, Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how trees can always fit the training data perfectly (as long as there are no identical X's with different y's), given that they can grow large enough. This becomes apparent in the training scores (the scores on the training data itself, rather than on the validation set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_dtc, parameter_names=['max_depth' ], train_scores=True)\n",
    "_ = plt.title('CV train scores, Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dtc.estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize a tree. Let's make a simple tree, with max_depth=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_tree = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "demo_tree.fit(X_train, y_train)\n",
    "fig, ax = plt.subplots(1,1, figsize=(12, 8))\n",
    "_ = plot_tree(demo_tree, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the simple Decision Tree Classifier, ROC-AUC close to 0.90 are feasible judging by the cross-validation scores. The variance is however quite high, so the final test score could disappoint us. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n",
    "\n",
    "The Naive Bayes classifier is a rather simple and powerful classifier, that has been used successfully in for instance spam filters. Here we will use a classifier that assumes a Gaussian distribution of its features, the Gaussian Naive Bayes classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "param_grid = {'nb__var_smoothing': np.logspace(-12, -3, num=4)} #Note the dictionary notation\n",
    "nb = GaussianNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_nb = GridSearchCV(pipeline, param_grid, cv=10, scoring='roc_auc', return_train_score=True) #NB: uses StratifiedKFold when cv=int\n",
    "_ = grid_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_nb, parameter_names=['nb__var_smoothing', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_nb, parameter_names=['nb__var_smoothing', ], train_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "\n",
    "Is the rather low performance of the Naive-Bayes the result of too high bias or variance? Can anything be done against this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is the classification-counterpart of Linear Least Squares for regression. Similar to linear regression, we can impose a penalty on larger coefficient values to prevent overfitting. This is called regularization. \n",
    "\n",
    "Too large a penalty (small C-value in the sklearn model) will lead to stable but sub-optimal performance (underfitting), too small a penalty may result in overfitting, especially when the number of features (columns) is high and the number of samples is low. When doing logistic regression, it is important to determine the optimal regularization strength in a cross-validation cycle. \n",
    "\n",
    "It is important that we scale the data before fitting the model when doing regularization (why?). The correct way is to make scaling a part of a cross-validation pipeline, which is done using the Pipeline class of sklearn. \n",
    "The Pipeline object will behave just as a single classifier, having .fit() and .predict() methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(solver='saga', n_jobs=-1, random_state=10))\n",
    "])\n",
    "# define the parameter grid, preceding the argument name with \"lr__\" when it applies to the LogisticRegression\n",
    "param_grid = {'lr__C': np.logspace(-5, -1, num=5), \n",
    "              #'lr__penalty': ['l1', 'l2']\n",
    "             } \n",
    "grid_lr = GridSearchCV(pipeline, param_grid, cv=5, return_train_score=True)\n",
    "grid_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_lr, parameter_names=['lr__C', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_lr, parameter_names=['lr__C', ], train_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "How would you improve this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifiers typically perform quite well over a wide range of parameters. The main parameter to tune is the depth of the individual trees ('max_depth'), which determines the model complexity. Typically, when the number of trees ('n_estimators') is chosen large enough (say, 100 or more), Random Forest classifiers do not easily overfit. This is because the classifier is an ensemble of many tree classifiers (\"bagging\"), which reduces the variance of the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [2, 5, 10, 20], 'n_estimators':[10, 100]}\n",
    "rfc = RandomForestClassifier(random_state=7) \n",
    "\n",
    "grid_rfc = GridSearchCV(rfc, param_grid, cv=5, scoring='roc_auc', return_train_score=True) \n",
    "_ = grid_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_rfc, parameter_names=['max_depth', 'n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_rfc, parameter_names=['max_depth', 'n_estimators'], train_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosted Trees\n",
    "\n",
    "Gradient boosted trees share some similarities with Random Forests, in that they are an ensemble of trees. Whereas a Random Forest classifier consists of trees grown individually, Gradient Boosting generates trees that successively address misclassifications of the previous trees. Although scikit-learn does have a Gradient Boosting implementation it is advised to use LightGBM, one of the most performant implementations in terms of speed and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm\n",
    "from sklearn.ensemble import GradientBoostingClassifier #scikit-learn implementation. Not advised\n",
    "from lightgbm import LGBMClassifier #roughly 2 orders of magnitude times faster than scikit-learn's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = LGBMClassifier()\n",
    "#clf_gb = GradientBoostingClassifier()\n",
    "param_grid = {\n",
    "    'max_depth':[2, 5, 10], # sklearn and lightgbm implementation\n",
    "    'num_iterations': [20, 50, 100], # lightgbm implementation\n",
    "    }\n",
    "\n",
    "grid_gb = GridSearchCV(estimator=clf_gb, param_grid=param_grid, cv=5, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_gb, parameter_names=['max_depth', 'num_iterations' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_gb, parameter_names=['max_depth', 'num_iterations' ], train_scores=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Which combination of parameters leads to strongest \"overfitting\"? \n",
    "- What is your preferred strategy for regularization\n",
    "- (Extra): do you expect these results to be repeatable? Which may be sources of non-repeatability?\n",
    "\n",
    "\n",
    "(NB: you could increase cv to 10 for better estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network\n",
    "\n",
    "A feedforward neural network consists of consecutive layers of densely connected neurons. Their weights and biases need to be trained using the training data. We make use of Tensorflow for speed of calculation, and keras as a wrapper around Tensorflow to make the construction of the neural network easier. We add dropout, a fast and efficient regularization technique.  Because dropout randomly disables neurons, more training rounds (\"epochs\") are needed and also a somewhat wider network architecture to achieve good results. \n",
    "\n",
    "Neural networks typically have a lot of parameters that can be tuned: the number of layers, the width of the layers, the activation functions to be used, the batch size, the optimizer, and in our case also the drop-out rate. \n",
    "\n",
    "We will take the default mini-batch size of 32, and scan for optimal dropout rate and network width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "KerasClassifier._estimator_type = 'classifier' #monkey-patch the class for plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf(width=4, dropout_rate=0.0):\n",
    "    ann = keras.models.Sequential()\n",
    "    ann.add(keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
    "    ann.add(keras.layers.Dense(units=width, activation='relu'))\n",
    "    ann.add(keras.layers.Dropout(rate=dropout_rate, noise_shape=None, seed=None))\n",
    "    ann.add(keras.layers.Dense(units=width, activation='relu'))\n",
    "    ann.add(keras.layers.Dropout(rate=dropout_rate, noise_shape=None, seed=None))\n",
    "    ann.add(keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "    ann.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                metrics=['accuracy', keras.metrics.AUC()])\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = KerasClassifier(build_fn=build_clf, epochs=3)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ann', ann)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'ann__width':[4, 6, 8],\n",
    "        'ann__dropout_rate' : [0.0, 0.25],\n",
    "        'ann__batch_size' : [32, ]\n",
    "        }\n",
    "grid_nn = GridSearchCV(estimator=pipeline, param_grid=params, cv=2, return_train_score=True, verbose=3)\n",
    "# now fit the dataset to the GridSearchCV object. \n",
    "_ = grid_nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_nn, parameter_names=['ann__width', 'ann__dropout_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?sns.boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "- How do the network width and drop-out rate affect performance? Is there an interaction? \n",
    "(you may need to increase cv to 5 to be able to answer this better)\n",
    "- Given the number of train samples (33'441), the minibatch-size (32) and number of epochs (3), how many iterations do you expect are needed per Crossvalidation-split? Does this match your observations? \n",
    "- (Extra): do you expect these results to be repeatable? Which may be sources of non-repeatability?\n",
    "NB: the final training passes are to generate a trained `._best_estimator`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Having optimized the hyperparameters of our chosen classifier in a cross-validation, we will use this classifier to generate our predictions. \n",
    "\n",
    "The most straightforward option is to use .best_estimator() to access the best performing classifier \n",
    "according to the cross-validation. Per default (as determined by the `refit` argument to the gridsearch object) the entire training data is used to fit this best estimator. \n",
    "\n",
    "We will use predefined functions that plot various curves for us and also report classification metrics. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data. \n",
    "y_test = pd.read_csv('y_test_supervised.csv.zip')['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test data (X_test) with our selected and optimized classifier \n",
    "# We will use the Gridsearch object that has the DecisionTreeClassifier, grid_dtc\n",
    "# Replace this object with yours\n",
    "y_pred_dtc = grid_dtc.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "print(f'The ROC-AUC test score: {roc_auc_score(y_test, y_pred_dtc):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional distribution of the predicted scores gives a visual understanding of how our classifier separates the actual positive and actual negative classes. Corresponding to the precision-recall curve, we see that it it is not possible to catch the majority of actual positives without having a large \"bycatch\" of actual negatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = plot_conditional_distribution(y_test, y_pred_dtc, bw=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even easier approach is to plot the ROC and  Precision-Recall curves using\n",
    "`plot_roc_curve` and `plot_precision_recall_curve`.\n",
    "Both functions require an estimator (we take our best one, stored in `.best_estimator_`), and the X_test and y_test data. See examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "_ = plot_roc_curve(estimator=grid_dtc.best_estimator_, X=X_test, y=y_test, ax=axs[0])\n",
    "_ = plot_precision_recall_curve(estimator=grid_dtc.best_estimator_, X=X_test, y=y_test, ax=axs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Witn `plot_conditional_distribution`, we can make conditional density plots, to visualize the degree of separation  between the positive and negative class by the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, take your best classifier and evaluate it and the same way. How does it compare against the simple decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_nn.best_estimator_.steps[-1][1]._estimator_type = 'classifier'\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "_ = plot_roc_curve(estimator=grid_nn.best_estimator_, X=X_test, y=y_test, ax=axs[0])\n",
    "_ = plot_precision_recall_curve(estimator=grid_nn.best_estimator_, X=X_test, y=y_test, ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "for clf in [grid_dtc, grid_nb, grid_lr, grid_rfc, grid_gb, grid_nn]:\n",
    "    label = (type(clf.estimator).__name__ if not type(clf.estimator).__name__ == 'Pipeline' \n",
    "        else type(clf.estimator[-1]).__name__)\n",
    "    _ = plot_roc_curve(estimator=clf.best_estimator_, X=X_test, y=y_test, ax=axs[0], label=label)\n",
    "    _ = plot_precision_recall_curve(estimator=clf.best_estimator_, X=X_test, y=y_test, ax=axs[1], label=label)\n",
    "    y_pred_dtc = clf.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "    print(f'The ROC-AUC test score of {label}: \\t {roc_auc_score(y_test, y_pred_dtc):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in [grid_dtc, grid_nb, grid_lr, grid_rfc, grid_gb, grid_nn]:\n",
    "    label = (type(clf.estimator).__name__ if not type(clf.estimator).__name__ == 'Pipeline' \n",
    "        else type(clf.estimator[-1]).__name__)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grid_lr.estimator[-1]).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/DonErnesto/masterclassSFI_2021/blob/main/notebooks/BitcoinSupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anti-Money Laundering with Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "\n",
    "The purpose of this Jupyter notebook is to make you familiar with several common supervised ML models, and to guide you through some essential steps when developing an ML model: hyperparameter tuning, model comparison and selection. \n",
    "\n",
    "Note that there are two types of cells in this notebook: **markdown cells** (that contain text, like this one), and **code cells** (that execute some code, like the next cell). \n",
    "\n",
    "By clicking the Play button on a cell, we execute a code cell. Lines that start with a \"#\" are comments, and not executed. Furthermore, note that correct **indentation** (use 4 spaces, always after a line that ends with a colon `:`) is mandatory.  \n",
    "\n",
    "\n",
    "The data we will be using was taken from Kaggle: https://www.kaggle.com/ellipticco/elliptic-data-set \n",
    "and describes blockchain transactions, some of which are flagged as \"illicit\" (i.e., relating to illegal activity), others as \"licit\" or \"unknown\" (unknown being the majority, with about 80%). We got rid of the unknown labels for simplicity. The authors give as examples of illicit categories: \"scams, malware, terrorist organizations, ransomware, Ponzi schemes, etc.\"\n",
    "\n",
    "We start by downloading the data we will be training on, which has already been splitted into \"X\" (features) and \"y\" (labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data import from Github\n",
    "import os\n",
    "force_download = False\n",
    "if force_download or not os.path.exists('X_train_supervised.csv.zip'): # then probably nothing was downloaded yet\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/ml_utils.py\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/X_train_supervised.csv.zip\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/y_train_supervised.csv.zip\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/X_test_supervised.csv.zip\n",
    "    !curl -O https://raw.githubusercontent.com/DonErnesto/masterclassSFI_2021/main/data/y_test_supervised.csv.zip\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using pandas for data handling, and scikit-learn (sklearn) for various helper functions, and for its supervised machine learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd # data I/O and manipulation\n",
    "import numpy as np # numeric operations\n",
    "import matplotlib.pyplot as plt # basic plotting\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve, plot_precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from ml_utils import grouped_boxplot_gridsearch, plot_conditional_distribution # our own helper functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the data in a so-called DataFrame (a pandas object), and inspect it by plotting the N-top rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train_supervised.csv.zip')\n",
    "X_test = pd.read_csv('X_test_supervised.csv.zip')\n",
    "y_train = pd.read_csv('y_train_supervised.csv.zip')['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .head() returns the first n (per default 5) rows of a DataFrame\n",
    "X_train.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We note that the feature extraction has been done for us, and that all\n",
    "# data has already been properly prepared (all values are numerical).\n",
    "# We therefore only remove unwanted features txId and Time step\n",
    "cols_to_drop = ['txId', 'Time step']\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further documentation on this dataset:**\n",
    "\n",
    "From the Kaggle-website ( https://www.kaggle.com/ellipticco/elliptic-data-set ): \"There are 166 features associated with each node. Due to intellectual property issues, we cannot provide an exact description of all the features in the dataset. There is a time step associated to each node, representing a measure of the time when a transaction was broadcasted to the Bitcoin network. ...\n",
    "\n",
    "The first 94 features represent local information about the transaction â€“ including the time step described above, number of inputs/outputs, transaction fee, output volume and aggregated figures such as average BTC received (spent) by the inputs/outputs and average number of incoming (outgoing) transactions associated with the inputs/outputs. The remaining 72 features are aggregated features, obtained using transaction information one-hop backward/forward from the center node - giving the maximum, minimum, standard deviation and correlation coefficients of the neighbour transactions for the same information data (number of inputs/outputs, transaction fee, etc.).\"\n",
    "\n",
    "Finally, and perhaps most relevant, the labels have been generated by a \"heuristics-based reasoning process\", as can be read in the article by Weber et al. (https://arxiv.org/pdf/1908.02591.pdf):\n",
    "\n",
    "\"For example, a higher number of inputs and the reuse of the same address is commonly associated with higher address-clustering, which results in a degrade of anonymity for the entity signing the transaction. On the other hand, consolidating funds controlled by multiple addresses in one single transaction provides benefits in terms of transaction costs (fee). It follows that entities eschewing anonymity-preserving measures for large volumes of user requests are likely to be licit (e.g. exchanges). In contrast, illicit activity may tend to favor transactions with a lower number of inputs to reduce the impact of de-anonymizing address-clustering techniques.\". \n",
    "\n",
    "This essentially means that the labels were generated by hand-crafted rules. It is thus possible, that these rules may be reconstructed by the model through proxy-features, and that a higher-than-realistic score is achieved. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of training data:', X_train.shape, '\\n')\n",
    "print('Label fractions:')\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f'\\nTest data size (fraction of total): {len(X_test)/(len(X_train) + len(X_test)):.2%} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 33.4k data points, of which 11% is a positive (which is quite a large fraction in a financial crime context). 28% of the data has been set aside for testing. \n",
    "\n",
    "We will not touch the test data until the very end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case-study: Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import the classes we want to use \n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting a question mark before (or after) a Python object shows you its documentation\n",
    "?DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we instantiate the DecisionTreeClassifier and define the parameter space we want to explore\n",
    "dtc = DecisionTreeClassifier(random_state=3) #Initialize with whatever parameters you want to\n",
    "\n",
    "# we will vary the maximum depth of the tree, and the minimum required number of samples to make a split\n",
    "param_grid = {'max_depth': [2, 5, 10, 20]} #Note the dictionary notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make use of the GridSearchCV estimator that does the scanning of the hyper-parameters for us,\n",
    "# in a k-Fold cross-validation loop\n",
    "grid_dtc = GridSearchCV(dtc, param_grid, cv=10, scoring='roc_auc', \n",
    "                        return_train_score=True) #NB: uses StratifiedKFold when cv=int\n",
    "\n",
    "# Finally, we fit the GridSearchCV estimator to our training data, using the .fit() method\n",
    "_ = grid_dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use grouped_boxplot_gridsearch (a self-made helper function) to show how classifier performance is affected by the various hyperparameter settings. \n",
    "\n",
    "Note that the boxplots show:\n",
    "- The median (\"mid-point\"), \n",
    "- a box spanning the first and third quartile, \n",
    "- and whiskers that extend to the median +/- 1.5 InterQuartile Range (IQR) or the lowest/highest point. Points beyond the median +/- 1.5 IQR are considered outliers and plotted explicitly\n",
    "\n",
    "<div>\n",
    "<img src=\"https://pro.arcgis.com/de/pro-app/2.7/help/analysis/geoprocessing/charts/GUID-0E2C3730-C535-40CD-8152-80D794A996A7-web.png\" width=\"200\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_dtc, parameter_names=['max_depth'])\n",
    "_ = plt.title('CV Test scores, Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how trees can always fit the training data perfectly (as long as there are no identical X's with different y's), given that they can grow large enough. This becomes apparent in the training scores (the scores on the training data itself, rather than on the validation set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_dtc, parameter_names=['max_depth' ], train_scores=True)\n",
    "_ = plt.title('CV Train scores, Decision Tree Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_dtc.estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's visualize a tree. Let's make a simple tree, with max_depth=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_tree = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "demo_tree.fit(X_train, y_train)\n",
    "fig, ax = plt.subplots(1,1, figsize=(12, 8))\n",
    "_ = plot_tree(demo_tree, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the simple Decision Tree Classifier, ROC-AUC close to 0.90 are feasible judging by the cross-validation scores. The variance is however quite high, so the final test score could disappoint us. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break-out session\n",
    "\n",
    "- Go the sub-section within `Model and hyper-parameter selection with cross-validation` belonging to your model and run the cells \n",
    "- For more accurate results you may increase `cv` to 5 (Neural Network) or 10 (others)\n",
    " \n",
    "- Answer the following questions:\n",
    "    - Do you see any signs of overfitting? (i.e. much better performance on the training than on the validation data)\n",
    "    - What can you say about the bias and variance of the model?\n",
    "    - What hyper-parameters help to regularize the model? In what direction?\n",
    "    - Which set of hyper-parameter values would you choose, and which one is chosen by the GridSearchCV object?\n",
    "- Go to the section `Evaluation on test data`, and evaluate the .best_estimator_ object within the fitted GridSearchCV object. This is the estimator with the highest mean cross-validation test metric (we have chosen ROC-AUC). \n",
    "    - What AUC do you get? Is it as expected based on your cross-validation results? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and hyper-parameter selection with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier\n",
    "\n",
    "The Naive Bayes classifier is a rather simple yet powerful classifier, that has been used successfully in for instance spam filters. Here we will use a classifier that assumes a Gaussian distribution of its features, the Gaussian Naive Bayes classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Naive Bayes classifier determines the mean and variance from the data. To avoid too small variances, the parameter `var_smoothing` boosts the variance of the features by epsilon, a fraction of the largest standard deviation of all features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('nb', GaussianNB())\n",
    "])\n",
    "param_grid = {'nb__var_smoothing': np.logspace(-12, -3, num=4)} #Note the dictionary notation\n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_nb = GridSearchCV(pipeline, param_grid, cv=10, scoring='roc_auc', return_train_score=True) #NB: uses StratifiedKFold when cv=int\n",
    "_ = grid_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_nb, parameter_names=['nb__var_smoothing', ])\n",
    "_ = plt.title('CV Test scores, Naive Bayes Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if desired, one can expect the detailed numerical results in the returned dataframe:\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_nb, parameter_names=['nb__var_smoothing', ], train_scores=True)\n",
    "_ = plt.title('CV Train scores, Naive Bayes Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is the classification-counterpart of Linear Least Squares for regression. Similar to linear regression, we can impose a penalty on larger coefficient values to prevent overfitting. This is called regularization. \n",
    "\n",
    "Too large a penalty (small C-value in the sklearn model) will lead to stable but sub-optimal performance (underfitting), too small a penalty may result in overfitting, especially when the number of features (columns) is high and the number of samples is low. When doing logistic regression, it is important to determine the optimal regularization strength in a cross-validation cycle. The default l2-penalty is chosen (penalizing the sum of squares of the coefficient values), this may be changed to a l1-penalty if desired. \n",
    "\n",
    "It is important that we scale the data before fitting the model when doing regularization. The correct way is to make scaling a part of a cross-validation pipeline, which is done using the Pipeline class of sklearn. \n",
    "The Pipeline object will behave just as a single classifier, having .fit() and .predict() methods. This means that  during cross-validation, also the parameters of the StandardScaler are fitted on the training split within the fold. \n",
    "\n",
    "**Question:** Why do you think the scaling is important when we regularize the model by penalizing the coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LogisticRegression(penalty='l2', solver='saga', n_jobs=-1, random_state=10))\n",
    "])\n",
    "# define the parameter grid, preceding the argument name with \"lr__\" when it applies to the LogisticRegression\n",
    "param_grid = {'lr__C': np.logspace(-5, -1, num=5), \n",
    "              #'lr__penalty': ['l1', 'l2']\n",
    "             } \n",
    "grid_lr = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "grid_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_lr, parameter_names=['lr__C', ])\n",
    "_ = plt.title('CV Test scores, Logistic Regression Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_lr, parameter_names=['lr__C', ], train_scores=True)\n",
    "_ = plt.title('CV Train scores, Logistic Regression Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifiers typically perform quite well over a wide range of parameters. The main parameter to tune is the depth of the individual trees ('max_depth'), which determines the model complexity. Typically, when the number of trees ('n_estimators') is chosen large enough (say, 100 or more), Random Forest classifiers typically do not suffer from overfitting. This is because the classifier is an ensemble of many tree classifiers (\"bagging\"), which reduces the variance of the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [2, 5, 10, 20], 'n_estimators':[10, 100]}\n",
    "rfc = RandomForestClassifier(random_state=7) \n",
    "\n",
    "grid_rfc = GridSearchCV(rfc, param_grid, cv=5, scoring='roc_auc', return_train_score=True) \n",
    "_ = grid_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_rfc, parameter_names=['max_depth', 'n_estimators'])\n",
    "_ = plt.title('CV Test scores, Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_rfc, parameter_names=['max_depth', 'n_estimators'], train_scores=True)\n",
    "_ = plt.title('CV Train scores, Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosted Trees\n",
    "\n",
    "Gradient boosted trees share some similarities with Random Forests, in that they are an ensemble of trees. Whereas a Random Forest classifier consists of trees grown individually, Gradient Boosting generates trees that successively address misclassifications of the previous trees. Although scikit-learn does have a Gradient Boosting implementation it is advised to use LightGBM, a very performant implementation in terms of speed and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm\n",
    "from sklearn.ensemble import GradientBoostingClassifier #scikit-learn implementation. Not advised\n",
    "from lightgbm import LGBMClassifier #roughly 2 orders of magnitude times faster than scikit-learn's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = LGBMClassifier()\n",
    "#clf_gb = GradientBoostingClassifier()\n",
    "param_grid = {\n",
    "    'max_depth':[2, 5, 10], # sklearn and lightgbm implementation\n",
    "    'num_iterations': [20, 50, 100], # lightgbm implementation\n",
    "    }\n",
    "\n",
    "grid_gb = GridSearchCV(estimator=clf_gb, param_grid=param_grid, cv=5, return_train_score=True, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_gb, parameter_names=['max_depth', 'num_iterations' ])\n",
    "_ = plt.title('CV Test scores, Gradient Boosted Tree Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_gb, parameter_names=['max_depth', 'num_iterations' ], train_scores=True)\n",
    "_ = plt.title('CV Train scores, Gradient Boosted Tree Classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network\n",
    "\n",
    "A feedforward neural network consists of consecutive layers of densely connected neurons. Their weights and biases need to be trained using the training data. We make use of Tensorflow for speed of calculation, and keras as a wrapper around Tensorflow to make the construction of the neural network easier. We add dropout, a fast and efficient regularization technique.  Because dropout randomly disables neurons, more training rounds (\"epochs\") are needed and also a somewhat wider network architecture to achieve good results. \n",
    "\n",
    "Neural networks typically have a lot of parameters that can be tuned: the number of layers, the width of the layers, the activation functions to be used, the batch size, the optimizer, and in our case also the drop-out rate. \n",
    "\n",
    "We will take the default mini-batch size of 32, and scan for optimal dropout rate and network width. \n",
    "\n",
    "Note how many steps and design choices there are to be made to build a -rather simple- neural network architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "KerasClassifier._estimator_type = 'classifier' #monkey-patch the class for plot_roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clf(width=4, dropout_rate=0.0):\n",
    "    ann = keras.models.Sequential()\n",
    "    ann.add(keras.layers.InputLayer(input_shape=(X_train.shape[1],)))\n",
    "    ann.add(keras.layers.Dense(units=width, activation='relu'))\n",
    "    ann.add(keras.layers.Dropout(rate=dropout_rate, noise_shape=None, seed=None))\n",
    "    ann.add(keras.layers.Dense(units=width, activation='relu'))\n",
    "    ann.add(keras.layers.Dropout(rate=dropout_rate, noise_shape=None, seed=None))\n",
    "    ann.add(keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "    ann.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                metrics=['accuracy', keras.metrics.AUC()])\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = KerasClassifier(build_fn=build_clf, epochs=3)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('ann', ann)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'ann__width':[4, 6, 8],\n",
    "        'ann__dropout_rate' : [0.0, 0.25],\n",
    "        'ann__batch_size' : [32, ]\n",
    "        }\n",
    "grid_nn = GridSearchCV(estimator=pipeline, param_grid=params, cv=2, return_train_score=True, verbose=3, \n",
    "                       scoring='roc_auc')\n",
    "# now fit the dataset to the GridSearchCV object. \n",
    "_ = grid_nn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped_boxplot_gridsearch(grid_nn, parameter_names=['ann__width', 'ann__dropout_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on test data\n",
    "\n",
    "Having optimized the hyperparameters of our chosen classifier in a cross-validation, we will use this classifier to generate our predictions. \n",
    "\n",
    "The most straightforward option is to use .best_estimator() to access the best performing classifier \n",
    "according to the cross-validation. Per default (as determined by the `refit` argument to the gridsearch object) the entire training data is used to fit this best estimator. \n",
    "\n",
    "We will use predefined functions that plot various curves for us and also report classification metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data. \n",
    "y_test = pd.read_csv('y_test_supervised.csv.zip')['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `plot_conditional_distribution`, we can make conditional density plots, to visualize the degree of separation  between the positive and negative class by the classifier. \n",
    "Replace `grid_dtc` with your own trained GridsearchCV object (`grid_xx`) for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = grid_dtc.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "print(f'The ROC-AUC test score: {roc_auc_score(y_test, y_pred):.3f}')\n",
    "_ = plot_conditional_distribution(y_test, y_pred, bw=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even easier approach is to plot the ROC and  Precision-Recall curves using\n",
    "`plot_roc_curve` and `plot_precision_recall_curve`.\n",
    "Both functions require an estimator (we take our best one, stored in `.best_estimator_`), and the X_test and y_test data. See examples below. \n",
    "\n",
    "Replace `grid_dtc` with your own trained GridsearchCV object (`grid_xx`) for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "_ = plot_roc_curve(estimator=grid_dtc.best_estimator_, X=X_test, y=y_test, ax=axs[0])\n",
    "_ = plot_precision_recall_curve(estimator=grid_dtc.best_estimator_, X=X_test, y=y_test, ax=axs[1])\n",
    "\n",
    "_ = axs[0].set_title('ROC Curve')\n",
    "_ = axs[1].set_title('Average Precision Curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of all \"best\" estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "for clf in [grid_dtc, grid_nb, grid_lr, grid_rfc, grid_gb, grid_nn]:\n",
    "    label = (type(clf.estimator).__name__ if not type(clf.estimator).__name__ == 'Pipeline' \n",
    "        else type(clf.estimator[-1]).__name__)\n",
    "    _ = plot_roc_curve(estimator=clf.best_estimator_, X=X_test, y=y_test, ax=axs[0], label=label)\n",
    "    _ = plot_precision_recall_curve(estimator=clf.best_estimator_, X=X_test, y=y_test, ax=axs[1], label=label)\n",
    "    y_pred = clf.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "    print(f'{label:>30}. ROC-AUC test score: {roc_auc_score(y_test, y_pred):>6.3f},   '\\\n",
    "         f'Average Precision test score: {average_precision_score(y_test, y_pred):>6.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
